{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## This jupyter file is all things grid dataset ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from spatial import MultipleGrid  ### Use this when MultipleGrid class is not directly being used\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cluster_coords(coordinates: pd.DataFrame, method: str, n_clusters: int):\n",
    "    if \"x\" not in coordinates.columns:\n",
    "        raise Exception(\"'x' must be in the input coordinate columns.\")\n",
    "    if \"y\" not in coordinates.columns:\n",
    "        raise Exception(\"'y' must be in the input coordinate columns.\")\n",
    "    if method not in [\"GaussianMixture\", \"KMeans\"]:\n",
    "        raise Exception(\"Inappropriate method.\")\n",
    "\n",
    "    np.random.seed(1000)\n",
    "    unique_coors = coordinates.drop_duplicates()\n",
    "    if method == 'GaussianMixture':\n",
    "        cluster_model = GaussianMixture(n_components=n_clusters).fit(unique_coors)\n",
    "    elif method == \"KMeans\":\n",
    "        cluster_model = KMeans(n_clusters=n_clusters).fit(unique_coors)\n",
    "    coor_pred = cluster_model.predict(unique_coors)\n",
    "    whole_pred = cluster_model.predict(coordinates)\n",
    "    coor_clusters = unique_coors.copy()\n",
    "    coor_clusters[\"cluster id\"] = coor_pred\n",
    "    whole_df = pd.DataFrame(whole_pred, index=coordinates.index, columns=[\"cluster id\"])\n",
    "    return whole_df, coor_clusters\n",
    "    \n",
    "def _split_in_cluster(coordinates: pd.DataFrame):\n",
    "    if \"x\" not in coordinates.columns:\n",
    "        raise Exception(\"'x' must be in the input coordinate columns.\")\n",
    "    if \"y\" not in coordinates.columns:\n",
    "        raise Exception(\"'y' must be in the input coordinate columns.\")\n",
    "\n",
    "    _, coor_nums = np.unique(coordinates, axis=0, return_counts=True)\n",
    "    large_num_coors = coordinates[coor_nums>=np.percentile(coor_nums,60)]\n",
    "    train_idx = np.random.choice(large_num_coors.index, size=len(coor_nums)//10)\n",
    "    train_coors = coordinates.loc[train_idx]\n",
    "    test_coors = coordinates.drop(train_idx)\n",
    "    return train_coors, test_coors\n",
    "\n",
    "def _split_train_test(xy_cluster: pd.DataFrame):\n",
    "    if type(xy_cluster) is not pd.DataFrame:\n",
    "        raise Exception(\"xy_cluster data is not pd.DataFrame.\")\n",
    "\n",
    "    np.random.seed(1000)\n",
    "    unique_xy_cluster = xy_cluster.drop_duplicates()\n",
    "    all_split_grids, all_split_data_id = {}, {}\n",
    "    for cluster in np.sort(pd.unique(unique_xy_cluster[\"cluster id\"])):\n",
    "        cluster_xy = unique_xy_cluster.loc[unique_xy_cluster[\"cluster id\"]==cluster, [\"x\",\"y\"]]\n",
    "        out_cluster_xy = unique_xy_cluster.loc[unique_xy_cluster[\"cluster id\"]!=cluster, [\"x\",\"y\"]].drop_duplicates()\n",
    "        cluster_train, cluster_test = _split_in_cluster(cluster_xy)\n",
    "        all_split_grids[cluster] = {\n",
    "            \"train_in_cluster\":cluster_train,\n",
    "            \"train_out_cluster\":out_cluster_xy,\n",
    "            \"test_cluster\":cluster_test\n",
    "        }\n",
    "        all_split_data_id[cluster] = {\n",
    "            \"train_in_cluster\":xy_cluster.index[np.isin(xy_cluster[[\"x\",\"y\"]], cluster_train).min(axis=1)],\n",
    "            \"train_out_cluster\":xy_cluster.index[np.isin(xy_cluster[[\"x\",\"y\"]], out_cluster_xy).min(axis=1)],\n",
    "            \"test_cluster\":xy_cluster.index[np.isin(xy_cluster[[\"x\",\"y\"]], cluster_test).min(axis=1)]\n",
    "        }\n",
    "    return all_split_grids, all_split_data_id\n",
    "\n",
    "def _split_train_test_nouq(xy_cluster: pd.DataFrame):\n",
    "    if type(xy_cluster) is not pd.DataFrame:\n",
    "        raise Exception(\"xy_cluster data is not pd.DataFrame.\")\n",
    "\n",
    "    np.random.seed(1000)\n",
    "#     unique_xy_cluster = xy_cluster.drop_duplicates()\n",
    "    all_split_grids, all_split_data_id = {}, {}\n",
    "    for cluster in np.sort(pd.unique(xy_cluster[\"cluster id\"])):\n",
    "        cluster_xy = xy_cluster.loc[xy_cluster[\"cluster id\"] == cluster, [\"x\",\"y\"]]\n",
    "        out_cluster_xy = xy_cluster.loc[xy_cluster[\"cluster id\"] != cluster, [\"x\",\"y\"]]\n",
    "        \n",
    "#         cluster_train, cluster_test = _split_in_cluster(cluster_xy)\n",
    "        all_split_grids[cluster] = {\n",
    "            \"train_in_cluster\":cluster_xy,\n",
    "            \"train_out_cluster\":out_cluster_xy,\n",
    "        }\n",
    "        all_split_data_id[cluster] = {\n",
    "            \"train_in_cluster\":xy_cluster.index[np.isin(xy_cluster[[\"x\",\"y\"]], cluster_xy).min(axis=1)],\n",
    "            \"train_out_cluster\":xy_cluster.index[np.isin(xy_cluster[[\"x\",\"y\"]], out_cluster_xy).min(axis=1)],\n",
    "        }\n",
    "    return all_split_grids, all_split_data_id\n",
    "\n",
    "class SingleGrid:\n",
    "    def __init__(self, cluster_method=\"GaussianMixture\", cluster_num=10):\n",
    "        self.cluster_num = cluster_num\n",
    "        self.cluster_method = cluster_method\n",
    "\n",
    "    def cluster_grids(self, input_dt: pd.DataFrame, target_dt: pd.Series):\n",
    "        if type(input_dt) is not pd.DataFrame:\n",
    "            raise Exception(\"Input data type is not pd.DataFrame.\")\n",
    "        if type(target_dt) is not pd.Series:\n",
    "            raise Exception(\"Target data type is not pd.Series.\")\n",
    "        if not input_dt.index.equals(target_dt.index):\n",
    "            raise Exception(\"Input and Output indexes are not equal.\")\n",
    "        if \"x\" not in input_dt.columns:\n",
    "            raise Exception(\"'x' must be in the input data columns.\")\n",
    "        if \"y\" not in input_dt.columns:\n",
    "            raise Exception(\"'y' must be in the input data columns.\")\n",
    "\n",
    "        return _cluster_coords(input_dt[[\"x\", \"y\"]], self.cluster_method, self.cluster_num)\n",
    "\n",
    "    def split_train_test(self, input_dt: pd.DataFrame, whole_cluster: pd.DataFrame):\n",
    "        if type(input_dt) is not pd.DataFrame:\n",
    "            raise Exception(\"Input data type is not pd.DataFrame.\")\n",
    "        if type(whole_cluster) is not pd.DataFrame:\n",
    "            raise Exception(\"Whole Cluster data type is not pd.DataFrame.\")\n",
    "\n",
    "        xy_cluster = input_dt[[\"x\", \"y\"]].join(whole_cluster)\n",
    "#         return _split_train_test(xy_cluster)\n",
    "        return _split_train_test_nouq(xy_cluster)\n",
    "        \n",
    "class MultipleGrid(SingleGrid):\n",
    "    def __init__(self, grid_scale, cluster_method=\"GaussianMixture\", cluster_num=10):\n",
    "        super().__init__(cluster_method, cluster_num)\n",
    "        self.grid_scale = grid_scale\n",
    "\n",
    "    def extract_center_data(self, tag_names: list, input_dt: np.ndarray):\n",
    "        if type(input_dt) is not np.ndarray:\n",
    "            raise Exception(\"Input data type is not np.array.\")\n",
    "        if input_dt.shape[1] != (len(tag_names)*(self.grid_scale**2)):\n",
    "            raise Exception(\"Input data column number is inappropriate.\")\n",
    "            \n",
    "        center_cell_id = (self.grid_scale**2)//2\n",
    "        center_dt = input_dt[:,center_cell_id*len(tag_names):(center_cell_id+1)*len(tag_names)]\n",
    "        return center_dt\n",
    "\n",
    "    def cluster_grids(self, tag_names: list, input_dt: np.ndarray, target_dt: pd.Series):\n",
    "        if type(input_dt) is not np.ndarray:\n",
    "            raise Exception(\"Input data type is not np.array.\")\n",
    "        if input_dt.shape[1] != (len(tag_names)*(self.grid_scale**2)):\n",
    "            raise Exception(\"Input data column number is inappropriate.\")\n",
    "        if type(target_dt) is not pd.Series:\n",
    "            raise Exception(\"Target data type is not pd.Series.\")\n",
    "        if len(input_dt) != len(target_dt):\n",
    "            raise Exception(\"The length of input data and target data are not equal.\")\n",
    "\n",
    "        center_dt = self.extract_center_data(tag_names, input_dt)\n",
    "        center_frame = pd.DataFrame(center_dt, columns=tag_names, index=target_dt.index)\n",
    "#         print(center_frame)\n",
    "        return super().cluster_grids(center_frame, target_dt)\n",
    "\n",
    "    def split_train_test(self, tag_names: list, input_dt: np.ndarray, whole_cluster: pd.DataFrame):\n",
    "        if type(input_dt) is not np.ndarray:\n",
    "            raise Exception(\"Input data type is not np.array.\")\n",
    "        if input_dt.shape[1] != (len(tag_names)*(self.grid_scale**2)):\n",
    "            raise Exception(\"Input data column number is inappropriate.\")\n",
    "        if type(whole_cluster) is not pd.DataFrame:\n",
    "            raise Exception(\"Whole Cluster data type is not pd.DataFrame.\")\n",
    "\n",
    "        center_dt = self.extract_center_data(tag_names, input_dt)\n",
    "        center_frame = pd.DataFrame(center_dt, columns=tag_names, index=whole_cluster.index)\n",
    "        return super().split_train_test(center_frame, whole_cluster)\n",
    "    \n",
    "    def generate_grid(self, index_list: list, whole_grid: np.ndarray, whole_label: np.ndarray):\n",
    "        if type(whole_grid) is not np.ndarray:\n",
    "            raise Exception(\"Grid data type is not np.array.\")\n",
    "        if whole_grid.shape[1] != (len(tag_names)*(self.grid_scale**2)):\n",
    "            raise Exception(\"Grid data column number is inappropriate.\")\n",
    "        \n",
    "        axis_val = 0\n",
    "        new_grid = np.take(whole_grid, index_list, axis_val)\n",
    "        new_label = np.take(whole_label, index_list, axis_val)\n",
    "        return new_grid, new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tags(tag_num):\n",
    "    first_tags = ['x', 'y']\n",
    "    rest_tags = [f'tag{x}' for x in range(tag_num-2)]\n",
    "    return first_tags + rest_tags\n",
    "\n",
    "def _get_train_test(cluster_train_test, cluster_id):\n",
    "    set_dt = cluster_train_test[cluster_id]\n",
    "    train_in = set_dt['train_in_cluster']\n",
    "    train_out = set_dt['train_out_cluster']\n",
    "    test_dt = set_dt['test_cluster']\n",
    "    return train_out, train_in, test_dt\n",
    "\n",
    "def _get_target_source(cluster_train_test, cluster_id):\n",
    "    set_dt = cluster_train_test[cluster_id]\n",
    "    train_in = set_dt['train_in_cluster']\n",
    "    train_out = set_dt['train_out_cluster']\n",
    "    return train_out, train_in\n",
    "\n",
    "\n",
    "def _plot_train_test(cluster_train_test, cluster_id):\n",
    "    set_dt = cluster_train_test[cluster_id]\n",
    "    train_in = set_dt['train_in_cluster']\n",
    "    train_out = set_dt['train_out_cluster']\n",
    "    test_dt = set_dt['test_cluster']\n",
    "    plt.scatter(train_out['x'], train_out['y'], 3, 'b', label='out-of-cluster train data')\n",
    "    plt.scatter(test_dt['x'], test_dt['y'], 3, 'r', label='test data')\n",
    "    plt.scatter(train_in['x'], train_in['y'], 5, 'g', label='in-cluster train data')\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "    plt.savefig(f'US_data/us-2011/clusters/cluster_{cluster_id}/cluster{cluster_id}train-test set')\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "\n",
    "def _plot_grid_clusters(coor_cluster):\n",
    "    plt.scatter(coor_cluster['x'], coor_cluster['y'], 3, coor_cluster['cluster id'])\n",
    "    plt.savefig('coordinate cluster')\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "\n",
    "### splitting the target dataset such that only k monitoring stations are included for transfer learning.\n",
    "def create_TL_targetsource_split(mn_df, train_test_grids, x_tr_blended, y_tr_blended, cluster_no):\n",
    "    source_cluster, target_cluster = _get_target_source(train_test_grids, cluster_no)\n",
    "    target_cluster_idxlist = target_cluster.index.values.tolist()\n",
    "#     mn_df_region = mn_df.iloc[target_cluster_idxlist]\n",
    "    \n",
    "    target_df = target_cluster.drop_duplicates(subset=[\"x\", \"y\"])\n",
    "    \n",
    "#     for idx in range(1,10):\n",
    "    idx = 1\n",
    "    temp_target_df = target_df.sample(n=10)\n",
    "    temp_test_df = target_df.loc[~target_df.index.isin(temp_target_df.index)]    \n",
    "\n",
    "    temp_target_df['loc_tuple'] = list(zip(temp_target_df.x, temp_target_df.y)) ### create tuple for x and y\n",
    "    target_df = target_cluster.merge(pd.DataFrame(temp_target_df['loc_tuple'].tolist(), columns=['x','y'])) ### find all the rows in target_df which are from temp_traget_df\n",
    "\n",
    "\n",
    "    temp_test_df['loc_tuple'] = list(zip(temp_test_df.x, temp_test_df.y)) \n",
    "    test_df = target_cluster.merge(pd.DataFrame(temp_test_df['loc_tuple'].tolist(), columns=['x','y'])) \n",
    "\n",
    "\n",
    "    ######### target grid data #########\n",
    "    target_cluster_list = target_df.index.values.tolist()\n",
    "    target_grid, target_label = multi_grid.generate_grid(target_cluster_list, x_tr_blended, y_tr_blended)\n",
    "\n",
    "    ftarget_grid = open(f'US_data/us-2011/target_california_grid_split/split-{idx}/target_grid_cal/target_cal.npy','wb')\n",
    "    np.save(ftarget_grid, target_grid)\n",
    "    ftarget_label = open(f'US_data/us-2011/target_california_grid_split/split-{idx}/target_grid_cal/target_cal_label.npy','wb')\n",
    "    np.save(ftarget_label, target_label)\n",
    "\n",
    "    ######### test grid data #########\n",
    "    test_cluster_list = test_df.index.values.tolist()\n",
    "    test_grid, test_label = multi_grid.generate_grid(test_cluster_list, x_tr_blended, y_tr_blended)\n",
    "\n",
    "    ftest_grid = open(f'US_data/us-2011/target_california_grid_split/split-{idx}/test_grid_cal/test_cal.npy','wb')\n",
    "    np.save(ftest_grid, test_grid)\n",
    "    ftest_label = open(f'US_data/us-2011/target_california_grid_split/split-{idx}/test_grid_cal/test_cal_label.npy','wb')\n",
    "    np.save(ftest_label, test_label)\n",
    "\n",
    "    ######### source grid data #########\n",
    "    source_cluster_list = source_cluster.index.values.tolist()\n",
    "    source_grid, source_label = multi_grid.generate_grid(source_cluster_list, x_tr_blended, y_tr_blended)\n",
    "\n",
    "    fsource_grid = open(f'US_data/us-2011/target_california_grid_split/split-{idx}/source_grid_cal/source_cal.npy','wb')\n",
    "    np.save(fsource_grid, source_grid)\n",
    "    fsource_label = open(f'US_data/us-2011/target_california_grid_split/split-{idx}/source_grid_cal/source_cal_label.npy','wb')\n",
    "    np.save(fsource_label, source_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700,)\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    data_path = 'US_data/BigUS/v10_170713_5x5_include_na_dataset.npz'\n",
    "    label_path = \"US_data/BigUS/v10_170713_5x5_include_na_label.npz\"\n",
    "    monitoring_df = pd.read_csv(\"US_Data/BigUS/us_monitoring.csv\")\n",
    "    \n",
    "    x_tr_blended = np.load(data_path)['arr_0']\n",
    "    y_tr_blended = np.load(label_path)['arr_0']\n",
    "    print(x_tr_blended[0].shape)\n",
    "    \n",
    "    tag_names = _create_tags(28)\n",
    "    multi_grid = MultipleGrid(5, \"KMeans\")    \n",
    "    whole_cluster, coor_cluster = multi_grid.cluster_grids(tag_names, x_tr_blended, pd.Series(y_tr_blended))\n",
    "#     print(coor_cluster)\n",
    "    \n",
    "# #     coor_cluster_x = coor_cluster[coor_cluster['cluster id'] == 4]\n",
    "# #     plt.figure(figsize=(10,6))\n",
    "# #     plt.scatter(coor_cluster['x'], coor_cluster['y'], 10, coor_cluster['cluster id'])\n",
    "# #     plt.show()\n",
    "    \n",
    "#     _plot_grid_clusters(coor_cluster) ### plotting the clusters (target-source: train and test)\n",
    "    train_test_grids, train_test_data_id = multi_grid.split_train_test(tag_names, x_tr_blended, whole_cluster)\n",
    "    create_TL_targetsource_split(monitoring_df, train_test_grids, x_tr_blended, y_tr_blended, 2)\n",
    "    \n",
    "    ##### get source-target-test clusters #####\n",
    "# #     source_cluster, target_cluster, test_cluster = _get_train_test(train_test_grids, 2)\n",
    "#     source_cluster, target_cluster = _get_target_source(train_test_grids, 2)\n",
    "#     print(target_cluster.shape)\n",
    "    \n",
    "#     target_cluster_list = target_cluster.index.values.tolist()\n",
    "#     target_grid, target_label = multi_grid.generate_grid(target_cluster_list, x_tr_blended, y_tr_blended)\n",
    "#     print(target_grid.shape)\n",
    "    \n",
    "# #     test_cluster_list = test_cluster.index.values.tolist()\n",
    "# #     test_grid = multi_grid.generate_grid(test_cluster_list, x_tr_blended)\n",
    "# #     print(test_grid.shape)\n",
    "    \n",
    "#     source_cluster_list = source_cluster.index.values.tolist()\n",
    "#     source_grid, source_label = multi_grid.generate_grid(source_cluster_list, x_tr_blended, y_tr_blended)\n",
    "#     print(source_grid.shape)\n",
    "\n",
    "        \n",
    "        ############################\n",
    "        \n",
    "#         test_cluster_list = test_cluster.index.values.tolist()\n",
    "#         test_grid, test_label = multi_grid.generate_grid(test_cluster_list, x_tr_blended, y_tr_blended)\n",
    "        \n",
    "#         ftest_grid = open(f'US_data/us-2011/grids/grid_{i}/test_{i}.npy','wb')\n",
    "#         np.save(ftest_grid, test_grid)\n",
    "#         ftest_label = open(f'US_data/us-2011/grids/grid_{i}/test_{i}_label.npy','wb')\n",
    "#         np.save(ftest_label, test_label)\n",
    "        \n",
    "#         ftest_cluster = open(f'US_data/us-2011/clusters/cluster_{i}/test_{i}.npy','wb')\n",
    "#         np.save(ftest_cluster, test_grid)\n",
    "#         ftest_label = open(f'US_data/us-2011/clusters/cluster_{i}/test_{i}_label.npy','wb')\n",
    "#         np.save(ftest_label, test_label)\n",
    "#         print(test_grid.shape)\n",
    "#         _plot_train_test(train_test_grids, i)\n",
    "        \n",
    "        ############################\n",
    "\n",
    "        \n",
    "\n",
    "############################ Creating the split for source-target-test clusters ############################\n",
    "#     for i in train_test_grids.keys():\n",
    "#         source_cluster, target_cluster = _get_target_source(train_test_grids, i)\n",
    "            \n",
    "#         target_cluster_list = target_cluster.index.values.tolist()\n",
    "#         target_grid, target_label = multi_grid.generate_grid(target_cluster_list, x_tr_blended, y_tr_blended)\n",
    "        \n",
    "#         ftarget_grid = open(f'US_data/us-2011/grids/grid_{i}/target_{i}.npy','wb')\n",
    "#         np.save(ftarget_grid, target_grid)\n",
    "#         ftarget_label = open(f'US_data/us-2011/grids/grid_{i}/target_{i}_label.npy','wb')\n",
    "#         np.save(ftarget_label, target_label)\n",
    "        \n",
    "#         ftarget_cluster = open(f'US_data/us-2011/clusters/cluster_{i}/target_{i}.npy','wb')\n",
    "#         np.save(ftarget_cluster, target_grid)\n",
    "#         ftarget_label = open(f'US_data/us-2011/clusters/cluster_{i}/target_{i}_label.npy','wb')\n",
    "#         np.save(ftarget_label, target_label)\n",
    "        \n",
    "#         print(target_grid.shape)\n",
    "\n",
    "#         source_cluster_list = source_cluster.index.values.tolist()\n",
    "#         source_grid, source_label = multi_grid.generate_grid(source_cluster_list, x_tr_blended, y_tr_blended)\n",
    "        \n",
    "#         fsource_grid = open(f'US_data/us-2011/grids/grid_{i}/source_{i}.npy','wb')\n",
    "#         np.save(fsource_grid, source_grid)\n",
    "#         fsource_label = open(f'US_data/us-2011/grids/grid_{i}/source_{i}_label.npy','wb')\n",
    "#         np.save(fsource_label, source_label)\n",
    "        \n",
    "#         fsource_cluster = open(f'US_data/us-2011/clusters/cluster_{i}/source_{i}.npy','wb')\n",
    "#         np.save(fsource_cluster, source_grid)\n",
    "#         fsource_label = open(f'US_data/us-2011/clusters/cluster_{i}/source_{i}_label.npy','wb')\n",
    "#         np.save(fsource_label, source_label)\n",
    "        \n",
    "#         print(source_grid.shape)\n",
    "#         print(\"-----------------------------\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>cmaq_x</th>\n",
       "      <th>cmaq_y</th>\n",
       "      <th>elev</th>\n",
       "      <th>emissi11_pm25</th>\n",
       "      <th>forest_cover</th>\n",
       "      <th>high</th>\n",
       "      <th>limi</th>\n",
       "      <th>local</th>\n",
       "      <th>...</th>\n",
       "      <th>narr_vgrd1815mb</th>\n",
       "      <th>narr_tmp30m</th>\n",
       "      <th>narr_pres2m</th>\n",
       "      <th>narr_pres10m</th>\n",
       "      <th>narr_pres30m</th>\n",
       "      <th>aod_value</th>\n",
       "      <th>pm25_value</th>\n",
       "      <th>gc_aod</th>\n",
       "      <th>pm25_value_k</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.256897e+05</td>\n",
       "      <td>3.587932e+05</td>\n",
       "      <td>35.77930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>873.759842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498815</td>\n",
       "      <td>294.425</td>\n",
       "      <td>101386.0</td>\n",
       "      <td>101282.0</td>\n",
       "      <td>101040.0</td>\n",
       "      <td>0.112333</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>8.582339</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1.571739e+06</td>\n",
       "      <td>3.949590e+05</td>\n",
       "      <td>1.01104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1050.015418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>807.793896</td>\n",
       "      <td>...</td>\n",
       "      <td>2.506630</td>\n",
       "      <td>298.217</td>\n",
       "      <td>101552.0</td>\n",
       "      <td>101482.0</td>\n",
       "      <td>101273.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.107923</td>\n",
       "      <td>7.383566</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.291484e+05</td>\n",
       "      <td>4.863733e+05</td>\n",
       "      <td>2.43082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1009.915135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496211</td>\n",
       "      <td>293.508</td>\n",
       "      <td>101352.0</td>\n",
       "      <td>101282.0</td>\n",
       "      <td>101073.0</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.022637</td>\n",
       "      <td>5.900033</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1.430635e+06</td>\n",
       "      <td>4.666680e+05</td>\n",
       "      <td>3.87558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2030.156665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.678500</td>\n",
       "      <td>295.279</td>\n",
       "      <td>97185.8</td>\n",
       "      <td>97082.1</td>\n",
       "      <td>96839.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.111803</td>\n",
       "      <td>7.488950</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1.573342e+06</td>\n",
       "      <td>4.646160e+05</td>\n",
       "      <td>3.35600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2037.573293</td>\n",
       "      <td>...</td>\n",
       "      <td>3.912880</td>\n",
       "      <td>296.529</td>\n",
       "      <td>101586.0</td>\n",
       "      <td>101515.0</td>\n",
       "      <td>101273.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.100897</td>\n",
       "      <td>7.398404</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249124</th>\n",
       "      <td>2011</td>\n",
       "      <td>365</td>\n",
       "      <td>-1.900296e+06</td>\n",
       "      <td>3.057078e+06</td>\n",
       "      <td>595.92000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.819380</td>\n",
       "      <td>290.432</td>\n",
       "      <td>100123.0</td>\n",
       "      <td>99985.6</td>\n",
       "      <td>99776.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.016803</td>\n",
       "      <td>10.765756</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249125</th>\n",
       "      <td>2011</td>\n",
       "      <td>365</td>\n",
       "      <td>-1.791970e+06</td>\n",
       "      <td>3.056591e+06</td>\n",
       "      <td>758.28300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.599889</td>\n",
       "      <td>288.057</td>\n",
       "      <td>96756.0</td>\n",
       "      <td>96652.2</td>\n",
       "      <td>96410.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.007157</td>\n",
       "      <td>9.456686</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249126</th>\n",
       "      <td>2011</td>\n",
       "      <td>365</td>\n",
       "      <td>-1.948693e+06</td>\n",
       "      <td>3.092715e+06</td>\n",
       "      <td>3.58333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.504280</td>\n",
       "      <td>288.724</td>\n",
       "      <td>102023.0</td>\n",
       "      <td>101886.0</td>\n",
       "      <td>101677.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.032277</td>\n",
       "      <td>11.428495</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249127</th>\n",
       "      <td>2011</td>\n",
       "      <td>365</td>\n",
       "      <td>-2.117447e+06</td>\n",
       "      <td>3.128790e+06</td>\n",
       "      <td>3.56685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.819380</td>\n",
       "      <td>290.349</td>\n",
       "      <td>102189.0</td>\n",
       "      <td>102019.0</td>\n",
       "      <td>101843.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.018143</td>\n",
       "      <td>12.663332</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249128</th>\n",
       "      <td>2011</td>\n",
       "      <td>365</td>\n",
       "      <td>-1.936944e+06</td>\n",
       "      <td>3.128039e+06</td>\n",
       "      <td>193.53500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.883379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.118860</td>\n",
       "      <td>290.037</td>\n",
       "      <td>100989.0</td>\n",
       "      <td>100952.0</td>\n",
       "      <td>100643.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.032277</td>\n",
       "      <td>11.476205</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249129 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  day        cmaq_x        cmaq_y       elev  emissi11_pm25  \\\n",
       "0       2011    1 -2.256897e+05  3.587932e+05   35.77930            0.0   \n",
       "1       2011    1  1.571739e+06  3.949590e+05    1.01104            0.0   \n",
       "2       2011    1 -1.291484e+05  4.863733e+05    2.43082            0.0   \n",
       "3       2011    1  1.430635e+06  4.666680e+05    3.87558            0.0   \n",
       "4       2011    1  1.573342e+06  4.646160e+05    3.35600            0.0   \n",
       "...      ...  ...           ...           ...        ...            ...   \n",
       "249124  2011  365 -1.900296e+06  3.057078e+06  595.92000            0.0   \n",
       "249125  2011  365 -1.791970e+06  3.056591e+06  758.28300            0.0   \n",
       "249126  2011  365 -1.948693e+06  3.092715e+06    3.58333            0.0   \n",
       "249127  2011  365 -2.117447e+06  3.128790e+06    3.56685            0.0   \n",
       "249128  2011  365 -1.936944e+06  3.128039e+06  193.53500            0.0   \n",
       "\n",
       "        forest_cover         high         limi        local  ...  \\\n",
       "0           0.000000     0.000000     0.000000   873.759842  ...   \n",
       "1           0.000000  1050.015418     0.000000   807.793896  ...   \n",
       "2           0.000000     0.000000     0.000000  1009.915135  ...   \n",
       "3           0.000000     0.000000  2030.156665     0.000000  ...   \n",
       "4           0.020202     0.000000     0.000000  2037.573293  ...   \n",
       "...              ...          ...          ...          ...  ...   \n",
       "249124      0.970588     0.000000     0.000000     0.000000  ...   \n",
       "249125      0.778547     0.000000     0.000000     0.000000  ...   \n",
       "249126      0.004456     0.000000     0.000000     0.000000  ...   \n",
       "249127      0.100092     0.000000     0.000000     0.000000  ...   \n",
       "249128      0.883379     0.000000     0.000000     0.000000  ...   \n",
       "\n",
       "        narr_vgrd1815mb  narr_tmp30m  narr_pres2m  narr_pres10m  narr_pres30m  \\\n",
       "0              0.498815      294.425     101386.0      101282.0      101040.0   \n",
       "1              2.506630      298.217     101552.0      101482.0      101273.0   \n",
       "2              0.496211      293.508     101352.0      101282.0      101073.0   \n",
       "3              3.678500      295.279      97185.8       97082.1       96839.6   \n",
       "4              3.912880      296.529     101586.0      101515.0      101273.0   \n",
       "...                 ...          ...          ...           ...           ...   \n",
       "249124         3.819380      290.432     100123.0       99985.6       99776.6   \n",
       "249125        -0.599889      288.057      96756.0       96652.2       96410.0   \n",
       "249126         1.504280      288.724     102023.0      101886.0      101677.0   \n",
       "249127         1.819380      290.349     102189.0      102019.0      101843.0   \n",
       "249128         3.118860      290.037     100989.0      100952.0      100643.0   \n",
       "\n",
       "        aod_value  pm25_value    gc_aod  pm25_value_k  month  \n",
       "0        0.112333         8.6  0.021297      8.582339    1.0  \n",
       "1             NaN         7.9  0.107923      7.383566    1.0  \n",
       "2        0.027000         5.9  0.022637      5.900033    1.0  \n",
       "3             NaN         5.4  0.111803      7.488950    1.0  \n",
       "4             NaN        10.2  0.100897      7.398404    1.0  \n",
       "...           ...         ...       ...           ...    ...  \n",
       "249124        NaN         4.7  0.016803     10.765756   12.0  \n",
       "249125        NaN         9.9  0.007157      9.456686   12.0  \n",
       "249126        NaN         3.9  0.032277     11.428495   12.0  \n",
       "249127        NaN         2.7  0.018143     12.663332   12.0  \n",
       "249128        NaN         6.3  0.032277     11.476205   12.0  \n",
       "\n",
       "[249129 rows x 78 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_monitoring = pd.read_csv(\"US_data/BigUS/us_monitoring.csv\")\n",
    "df_monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [year, day, cmaq_x, cmaq_y, elev, emissi11_pm25, forest_cover, high, limi, local, is, pd, lon, lat, rid, elev_k, emissi11_pm25_k, emissi11_pm10_k, forest_cover_k, high_k, limi_k, local_k, is_k, pd_k, cmaq_id, nldas_pevapsfc, nldas_dlwrfsfc, nldas_dswrfsfc, nldas_cape, nldas_fpcsfc, nldas_pcpsfc, nldas_rh2m, nldas_tmp2m, nldas_vgrd10m, nldas_ugrd10m, nldas_pressfc, narr_dpt, narr_vis, narr_hpbl, narr_rh2m, narr_tmp2m, narr_ugrd10m, narr_vgrd10m, narr_rh30mb, narr_rh63mb, narr_rh96mb, narr_rh129mb, narr_rh1512mb, narr_rh1815mb, narr_tmp30mb, narr_tmp63mb, narr_tmp96mb, narr_tmp129mb, narr_tmp1512mb, narr_tmp1815mb, narr_ugrd30m, narr_ugrd30mb, narr_ugrd63mb, narr_ugrd96mb, narr_ugrd129mb, narr_ugrd1512mb, narr_ugrd1815mb, narr_vgrd30m, narr_vgrd30mb, narr_vgrd63mb, narr_vgrd96mb, narr_vgrd129mb, narr_vgrd1512mb, narr_vgrd1815mb, narr_tmp30m, narr_pres2m, narr_pres10m, narr_pres30m, aod_value, pm25_value, gc_aod, pm25_value_k, month]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 78 columns]\n"
     ]
    }
   ],
   "source": [
    "df_mon_x = df_monitoring[df_monitoring['cmaq_x'] == -1.80717547e+06]\n",
    "# print(df_mon_x[df_mon_x['cmaq_y'] == 1.28267051e+06])\n",
    "print(df_mon_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
